{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7fca5f",
   "metadata": {},
   "source": [
    "# **Data Cleaning and Preparation for Original Dataset**\n",
    "### **1. Introduction**\n",
    "\n",
    "This report details the systematic data cleaning and feature selection process applied to the raw Global Terrorism Database (GTD). The primary objective of this procedure is to transform the original, complex dataset into a clean, focused, and high-quality dataset specifically tailored for a machine learning model. The Global Terrorism Database (GTD) is a large and complex dataset containing terrorism incidents from 1970 to 2020, with 135 columns in its raw form (globalterrorismdb_0522dist_ML.csv). The goal of this preprocessing is to create a clean, model-ready dataset (globalterrorismdb_cleaned.csv) with a focused set of features for predicting the terrorist group responsible (GroupName).\n",
    "\n",
    "### **2. The Raw Dataset: Initial State**\n",
    "The raw dataset is complex, with many columns containing >20% missing values, inconsistent naming, and irrelevant features for predicting GroupName. Data cleaning is essential to:\n",
    "- **Reduce Complexity**: Drop columns with high missing values to make the dataset manageable (from 135 to 47 columns, then to 11 after feature selection).\n",
    "- **Eliminate Noise**: Remove unreliable columns to prevent bias in modeling and visualization.\n",
    "- **Ensure Relevance**: Focus on 10 predictive features (Year, Month, Country, Region, Latitude, Longitude, AttackType, TargetType, WeaponType, suicide) and the target (GroupName).\n",
    "- **Standardize Data**: Rename columns for clarity (e.g., country_txt to Country) and impute missing values to ensure a complete dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996c71b",
   "metadata": {},
   "source": [
    "#### ***Step 1: Load and Rename Data***\n",
    "- **Purpose**: Load the raw dataset and rename columns for clarity and consistency.\n",
    "- **Why**: The original column names (e.g., year, country_txt, attacktype_p_txt) are inconsistent or unclear. Renaming them to intuitive names (e.g., Year, Country, AttackType) improves readability and ensures compatibility with downstream analysis. The latin-1 encoding handles special characters in the data, and low_memory=False prevents memory issues during loading due to the dataset's size.\n",
    "- **Rationale**: Clear column names reduce errors in coding and make the dataset more accessible for analysts. Standardizing names early ensures consistency across all subsequent steps.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Renaming ---\n",
      "Dataset loaded and columns renamed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"--- Step 1: Loading and Renaming ---\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('globalterrorismdb_0522dist_ML.csv', encoding='latin-1', low_memory=False)\n",
    "\n",
    "# Rename key columns for clarity and easy access\n",
    "df.rename(columns={\n",
    "    'iyear':'Year', 'imonth':'Month', 'gname':'GroupName', 'country_txt':'Country',\n",
    "    'region_txt': 'Region', 'latitude':'Latitude', 'longitude':'Longitude',\n",
    "    'attacktype1_txt':'AttackType', 'targtype1_txt':'TargetType',\n",
    "    'weaptype1_txt':'WeaponType', 'nkill':'Killed', 'nwound':'Wounded'\n",
    "}, inplace=True)\n",
    "print(\"Dataset loaded and columns renamed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164bf5e",
   "metadata": {},
   "source": [
    "#### ***Step 2: Define Features and Target***\n",
    "- **Purpose**: Select a subset of features (Year, Month, Country, Region, Latitude, Longitude, AttackType, TargetType, WeaponType, suicide) and the target variable (GroupName) for the prediction task.\n",
    "- **Why**: The goal is to predict the terrorist group responsible for an incident based on its characteristics. These 10 features capture the temporal, geographical, and attack-specific attributes likely to be predictive of GroupName. Other columns (e.g., IDs, narrative text) are excluded to avoid data leakage or irrelevant information.\n",
    "- **Rationale**: Focusing on a small, relevant feature set reduces computational complexity, mitigates the risk of overfitting, and aligns with the modeling objective. Explicitly defining the target and features ensures clarity in the pipeline.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c2b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Defining Features based on the Model Goal ---\n",
      "Prediction Target (y): GroupName\n",
      "Core Predictor Features (X): ['Year', 'Month', 'Country', 'Region', 'Latitude', 'Longitude', 'AttackType', 'TargetType', 'WeaponType', 'suicide']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Step 2: Defining Features based on the Model Goal ---\")\n",
    "# For predicting the 'GroupName', we select features that describe the attack's signature.\n",
    "\n",
    "# These are the CORE features we want to keep.\n",
    "features_to_keep = [\n",
    "    'Year', 'Month', 'Country', 'Region', 'Latitude', 'Longitude',\n",
    "    'AttackType', 'TargetType', 'WeaponType', 'suicide'\n",
    "]\n",
    "\n",
    "# This is our prediction target.\n",
    "target_variable = 'GroupName'\n",
    "\n",
    "print(f\"Prediction Target (y): {target_variable}\")\n",
    "print(f\"Core Predictor Features (X): {features_to_keep}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea777",
   "metadata": {},
   "source": [
    "#### ***Step 3: Automated Cleaning (Drop High-Missing Columns)***\n",
    "- **Purpose**: Drop columns with more than 20% missing values to reduce noise and complexity.\n",
    "- **Why**: The raw dataset has 135 columns, many of which have high missingness (e.g., >20%). Retaining these columns would require extensive imputation, which could introduce bias or unreliable data into the model. Dropping them reduces the dataset to 47 columns, making it more manageable while preserving most relevant information.\n",
    "- **Rationale**: A 20% threshold balances retaining useful data with eliminating unreliable columns. This automated approach is efficient for large datasets, as manually inspecting 135 columns is impractical. Dropping high-missing columns early prevents downstream issues in modeling and visualization.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11dbc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Automated Cleaning ---\n",
      "Original number of columns: 135\n",
      "Columns dropped for >20% missing. New number of columns: 47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Step 3: Automated Cleaning ---\")\n",
    "print(f\"Original number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Rule: Drop any column with more than 20% missing values.\n",
    "missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "cols_to_drop_auto = missing_percentage[missing_percentage > 20].keys()\n",
    "df.drop(columns=cols_to_drop_auto, inplace=True)\n",
    "\n",
    "print(f\"Columns dropped for >20% missing. New number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf6a29",
   "metadata": {},
   "source": [
    "#### ***Step 4: Create Final Focused Dataset***\n",
    "- **Purpose**: Create a new DataFrame (model_df) containing only the 10 selected features and the target (GroupName), implicitly dropping all other columns.\n",
    "- **Why**: After automated cleaning, the dataset still contains 47 columns, many of which are irrelevant for predicting GroupName. Selecting only the 11 needed columns (10 features + 1 target) ensures a focused dataset, reducing memory usage and eliminating potential data leakage (e.g., from summary text or ID columns).\n",
    "- **Rationale**: Using copy() avoids SettingWithCopyWarning, ensuring safe DataFrame operations. This step streamlines the dataset for EDA and modeling, focusing on features that describe the attack's signature.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3af491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Creating the Final Model-Ready Dataset ---\n",
      "Created a new DataFrame with only the 11 required columns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Step 4: Creating the Final Model-Ready Dataset ---\")\n",
    "\n",
    "# We create our final dataset using only the features we need.\n",
    "# This implicitly drops all other columns (IDs, text summaries, data leakage columns, etc.)\n",
    "all_needed_cols = features_to_keep + [target_variable]\n",
    "model_df = df[all_needed_cols].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "print(f\"Created a new DataFrame with only the {len(model_df.columns)} required columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f395b",
   "metadata": {},
   "source": [
    "#### ***Step 5: Final Imputation***\n",
    "- **Purpose**: Handle remaining missing values in the selected features by imputing numerical columns (Latitude, Longitude) with their mean and categorical columns (Country, Region, AttackType, TargetType, WeaponType, GroupName) with their mode.\n",
    "- **Why**: Even after dropping high-missing columns, some selected features may have minor missing values. Imputing with the mean for numerical columns preserves the central tendency of geographical coordinates, while using the mode for categorical columns ensures the most common category is used, which is robust for small amounts of missing data.\n",
    "- **Rationale**: Simple imputation methods (mean, mode) are chosen to avoid introducing complex assumptions or bias, given the large dataset size (289,796 rows). This ensures a complete dataset with no missing values, ready for modeling and visualization.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Handling Remaining Missing Values ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush Gupte\\AppData\\Local\\Temp\\ipykernel_1968\\3078412390.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  model_df['Latitude'].fillna(model_df['Latitude'].mean(), inplace=True)\n",
      "C:\\Users\\Ayush Gupte\\AppData\\Local\\Temp\\ipykernel_1968\\3078412390.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  model_df['Longitude'].fillna(model_df['Longitude'].mean(), inplace=True)\n",
      "C:\\Users\\Ayush Gupte\\AppData\\Local\\Temp\\ipykernel_1968\\3078412390.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  model_df[col].fillna(model_df[col].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final imputation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Step 5: Handling Remaining Missing Values ---\")\n",
    "# Even after dropping columns, some of our selected features might have a few missing values.\n",
    "# We will impute them with simple, robust methods.\n",
    "\n",
    "# For numerical columns like Latitude/Longitude, fill with the mean.\n",
    "model_df['Latitude'].fillna(model_df['Latitude'].mean(), inplace=True)\n",
    "model_df['Longitude'].fillna(model_df['Longitude'].mean(), inplace=True)\n",
    "\n",
    "# For any categorical columns, fill with the mode (most frequent value).\n",
    "for col in model_df.select_dtypes(include='object').columns:\n",
    "    model_df[col].fillna(model_df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"Final imputation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a83e9",
   "metadata": {},
   "source": [
    "#### ***Step 6: Verify the Cleaned Dataset***\n",
    "- **Purpose**: Confirm that the final dataset has no missing values, correct data types, and the expected structure (11 columns, 289,796 rows).\n",
    "- **Why**: Verification ensures the preprocessing steps were successful, checking for data integrity (e.g., no missing values) and correct formatting (e.g., suicide as int, Latitude as float). Displaying the first 5 rows provides a quick visual check of the data.\n",
    "- **Rationale**: This step catches any errors introduced during preprocessing, ensuring the dataset is model-ready and suitable for EDA or machine learning.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162563e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Verifying the Final Dataset ---\n",
      "Dataset Info (should show no missing values):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209706 entries, 0 to 209705\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   Year        209706 non-null  int64  \n",
      " 1   Month       209706 non-null  int64  \n",
      " 2   Country     209706 non-null  object \n",
      " 3   Region      209706 non-null  object \n",
      " 4   Latitude    209706 non-null  float64\n",
      " 5   Longitude   209706 non-null  float64\n",
      " 6   AttackType  209706 non-null  object \n",
      " 7   TargetType  209706 non-null  object \n",
      " 8   WeaponType  209706 non-null  object \n",
      " 9   suicide     209706 non-null  int64  \n",
      " 10  GroupName   209706 non-null  object \n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 17.6+ MB\n",
      "\n",
      "First 5 rows of the final, model-ready dataset:\n",
      "   Year  Month             Country                       Region   Latitude  \\\n",
      "0  1970      7  Dominican Republic  Central America & Caribbean  18.456792   \n",
      "1  1970      0              Mexico                North America  19.371887   \n",
      "2  1970      1         Philippines               Southeast Asia  15.478598   \n",
      "3  1970      1              Greece               Western Europe  37.997490   \n",
      "4  1970      1               Japan                    East Asia  33.580412   \n",
      "\n",
      "    Longitude                      AttackType                   TargetType  \\\n",
      "0  -69.951164                   Assassination  Private Citizens & Property   \n",
      "1  -99.086624     Hostage Taking (Kidnapping)      Government (Diplomatic)   \n",
      "2  120.599741                   Assassination          Journalists & Media   \n",
      "3   23.762728               Bombing/Explosion      Government (Diplomatic)   \n",
      "4  130.396361  Facility/Infrastructure Attack      Government (Diplomatic)   \n",
      "\n",
      "   WeaponType  suicide                           GroupName  \n",
      "0     Unknown        0                              MANO-D  \n",
      "1     Unknown        0  23rd of September Communist League  \n",
      "2     Unknown        0                             Unknown  \n",
      "3  Explosives        0                             Unknown  \n",
      "4  Incendiary        0                             Unknown  \n",
      "\n",
      "Dataset is now clean, formatted, and ready for model training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Step 6: Verifying the Final Dataset ---\")\n",
    "print(\"Dataset Info (should show no missing values):\")\n",
    "model_df.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows of the final, model-ready dataset:\")\n",
    "print(model_df.head())\n",
    "\n",
    "print(\"\\nDataset is now clean, formatted, and ready for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c76d1d",
   "metadata": {},
   "source": [
    "#### ***Step 7: Save the Cleaned Dataset***\n",
    "- **Purpose**: Save the cleaned DataFrame (model_df) to a new CSV file (globalterrorismdb_cleaned.csv) without the index.\n",
    "(lbl) column.\n",
    "- **Why**: The cleaned dataset is saved for use in subsequent analysis or modeling tasks, ensuring reproducibility and ease of access. Excluding the index prevents unnecessary columns in the output file.\n",
    "- **Rationale**: Saving the dataset preserves the preprocessing work, allowing analysts to load the clean dataset directly for further tasks, such as EDA or model training.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ebbfedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cleaned data has been successfully saved as 'globalterrorismdb_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Add this code to the end of your script ---\n",
    "\n",
    "# Define the name for your new, clean CSV file\n",
    "cleaned_file_name = 'globalterrorismdb_cleaned.csv'\n",
    "\n",
    "# Save the model_df DataFrame to the new file\n",
    "# index=False prevents pandas from writing the DataFrame index as a column\n",
    "model_df.to_csv(cleaned_file_name, index=False)\n",
    "\n",
    "print(f\"\\n✅ Cleaned data has been successfully saved as '{cleaned_file_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2f998",
   "metadata": {},
   "source": [
    "#### ***Why Dropping Columns with High Missing Values is Critical***\n",
    "- **Handling Large Datasets**: With 135 columns and 289,796 rows, the dataset is computationally intensive. Columns with >20% missing values (e.g., detailed text fields or obscure metadata) are often incomplete and add little predictive value, increasing processing time and memory usage.\n",
    "- **Reducing Noise**: High-missing columns can introduce noise or bias if imputed improperly, leading to unreliable model predictions or misleading visualizations.\n",
    "- **Efficiency**: Dropping irrelevant or sparse columns reduces the dataset to a manageable size (47 columns initially, then 11 after feature selection), enabling faster processing and analysis.\n",
    "- **Focus on Predictive Features**: The selected features (Year, Month, Country, etc.) are chosen for their relevance to predicting GroupName. Dropping other columns ensures the dataset is tailored to the modeling goal, improving model performance and interpretability.\n",
    "#\n",
    "This preprocessing pipeline transforms a large, messy dataset into a clean, focused, and model-ready dataset, enabling effective EDA and accurate machine learning predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232dbe1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
